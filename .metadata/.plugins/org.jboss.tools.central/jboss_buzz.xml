<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>How to debug C++ lambda expressions with GDB</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/03/how-debug-c-lambda-expressions-gdb" /><author><name>Kevin Buettner</name></author><id>f017049c-3b61-4400-b158-a767e759946e</id><updated>2023-05-03T07:00:00Z</updated><published>2023-05-03T07:00:00Z</published><summary type="html">&lt;p&gt;Modern versions of the &lt;a href="https://developers.redhat.com/topics/c"&gt;C++&lt;/a&gt; programming language have a feature known as &lt;a href="https://en.cppreference.com/w/cpp/language/lambda"&gt;lambda expressions&lt;/a&gt;. This article shows how you can debug lambda expressions using GDB, the GNU Project Debugger. Even if you're not interested in debugging lambdas, the techniques presented here are useful for many other debugging situations.&lt;/p&gt; &lt;h2&gt;What is a lambda expression?&lt;/h2&gt; &lt;p&gt;A lambda expression provides the C++ programmer with a way to create an anonymous or unnamed function. Lambda expressions are often used in situations where a callback function is desired. Using a lambda expression often makes writing a callback function significantly easier since various pieces of state that are known in the function from which the callback is passed don't need to be packaged up into a data structure which the callback function will later access. This is due to the fact that a C++ lambda expression provides a way to capture in-scope variables which may be later used when the lambda expression is executed.&lt;/p&gt; &lt;p&gt;The example below shows some simple captures in a few of its lambda expressions.&lt;/p&gt; &lt;h2&gt;Example program&lt;/h2&gt; &lt;p&gt;I'll use the example program below to demonstrate some debugging techniques in addition to showing some of the challenges that might be encountered when debugging lambda expressions. I've included line numbers as comments for some of the lines at which breakpoints might be placed. I've named this program &lt;code&gt;lambda.cc&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;#include &lt;stdio.h&gt; #include &lt;functional&gt; int successor (int i) { return i + 1; } int apply0 (int (*fn)(int), int arg) { return fn (arg); } int apply1 (std::function&lt;int(int)&gt; fn, int arg) { return fn (arg); } std::function&lt;int(int)&gt; make_function(int&amp; x) { return [&amp;] (int i) { return i + x; }; /* Line 17 */ } int main (int argc, char **argv) { int n = 7, m = -28; printf ("Answer 1 is %d\n", apply0 (successor, 3)); /* Line 24 */ printf ("Answer 2 is %d\n", apply1 (successor, 4)); /* Line 25 */ printf ("Answer 3 is %d\n", apply0 ([] (int i) { return i + 1; }, 1)); /* Line 28 */ printf ("Answer 4 is %d\n", apply1 ([] (int i) { return i + 1; }, 2)); /* Line 30 */ printf ("Answer 5 is %d\n", apply1 ([n] (int i) { return i + n; }, 4)); /* Line 33 */ auto lf2 = make_function (n); /* Line 35 */ printf ("Answer 6 is %d\n", apply1 (lf2, 1)); /* Line 36 */ auto lf3 = make_function (m); printf ("Answer 7 is %d\n", apply1 (lf3, -14)); /* Line 39 */ } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Lines 17, 28, 30, and 33 all contain lambda expressions. The lambda expression on line 33 is:&lt;/p&gt; &lt;p&gt;&lt;code&gt;[n] (int i) { return i + n; }&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Lambda expressions start with a left square bracket. In this particular lambda expression the left square bracket is followed by &lt;code&gt;n&lt;/code&gt;, which is a variable that is being &lt;em&gt;captured&lt;/em&gt; for use in the body of the lambda. Note that &lt;code&gt;n&lt;/code&gt; is a local variable in the function &lt;code&gt;main&lt;/code&gt;. Parentheses enclose a list of formal parameters to the anonymous function being defined; in this case there is just one parameter named &lt;code&gt;i&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Finally, the body of the lambda expression is placed between curly braces, just like a normal function. The body consists of zero or more executable statements. In this case, there is just one executable statement, a return statement. But do note that the expression to return refers to both the captured variable &lt;code&gt;n&lt;/code&gt; and the parameter &lt;code&gt;i&lt;/code&gt;. Note that there is no function name—that's what makes it anonymous. Lambda expressions are sometimes assigned to variables; this is done indirectly on lines 35 and 38. There are other optional syntactic components as well as many nuances of lambda expressions which are not described here.&lt;/p&gt; &lt;h2&gt;Building and debugging the example program&lt;/h2&gt; &lt;p&gt;You can use the GNU C++ compiler to compile and link the example program by using this command:&lt;/p&gt; &lt;p&gt;&lt;code&gt;g++ -Wall -g -o lambda lambda.cc&lt;/code&gt;&lt;/p&gt; &lt;p&gt;A similar command can be used to build an executable using the LLVM C++ compiler (clang++):&lt;/p&gt; &lt;p&gt;&lt;code&gt;clang++ -Wall -g -o lambda-clang lambda.cc &lt;/code&gt;&lt;/p&gt; &lt;p&gt;GDB interactions shown below were performed using GDB 13.1 on a Fedora 37 machine. Except where noted, the example program was compiled using GCC (g++) 12.2.1. When I did use LLVM (clang++) to check compatibility, I used Clang version 15.0.7.&lt;/p&gt; &lt;p&gt;We can begin debugging the example program using GDB as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ gdb -q lambda Reading symbols from lambda... (gdb) start Temporary breakpoint 1 at 0x401275: file lambda.cc, line 22. Starting program: /home/kev/examples/lambda This GDB supports auto-downloading debuginfo from the following URLs: &lt;https://debuginfod.fedoraproject.org/&gt; Enable debuginfod for this session? (y or [n]) y Debuginfod has been enabled. To make this setting permanent, add 'set debuginfod enabled on' to .gdbinit. [Thread debugging using libthread_db enabled] Using host libthread_db library "/lib64/libthread_db.so.1". Temporary breakpoint 1, main (argc=1, argv=0x7fffffffdd88) at lambda.cc:22 22 int n = 7, m = -28; (gdb) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This example shows how GDB is invoked on the executable named &lt;code&gt;lambda&lt;/code&gt;. The &lt;code&gt;-q&lt;/code&gt; switch causes GDB to not display copyright, warranty and information about how to obtain documentation and help. Once in GDB, the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Starting.html#index-start"&gt;&lt;code&gt;start&lt;/code&gt; command&lt;/a&gt; is used to run to the first executable line in the &lt;code&gt;main&lt;/code&gt; function.&lt;/p&gt; &lt;p&gt;Note, too, that I answered &lt;code&gt;y&lt;/code&gt; to enable &lt;a href="https://developers.redhat.com/blog/2019/10/14/introducing-debuginfod-the-elfutils-debuginfo-server"&gt;debuginfod&lt;/a&gt; for the session. When debuginfod is enabled, debugging information associated with libraries used by the program may be downloaded for use by GDB.&lt;/p&gt; &lt;p&gt;In the following examples, I won't show (again) the initial step of compiling the program nor the initial steps of debugging the program with GDB. That said, if you want to follow along, I've organized the examples (up to the section &lt;a href="#Debugging an LLVM compiled program"&gt;Debugging an LLVM compiled program&lt;/a&gt;) so that you won't need to restart GDB—you should be able to obtain similar output by simply typing the commands shown here to GDB.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; if the version of GDB or version of the compiler used to build the example program differ significantly form the versions noted earlier, it's possible that you'll see output different from what I show here.&lt;/p&gt; &lt;h2&gt;Debugging apply0 and successor&lt;/h2&gt; &lt;p&gt;Before looking at debugging a lambda expression, let's first look at using GDB to step into the &lt;code&gt;apply0&lt;/code&gt; call at line 24. Note that &lt;code&gt;successor&lt;/code&gt; is passed to &lt;code&gt;apply0&lt;/code&gt;; the &lt;code&gt;successor&lt;/code&gt; function adds one to its &lt;code&gt;int&lt;/code&gt; argument and returns this value.  &lt;code&gt;apply0&lt;/code&gt; takes two arguments, the first of which is a pointer to a function with the second being the value to pass to that function.  It simply calls the function with the argument and returns the result. Stepping into &lt;code&gt;apply0&lt;/code&gt; and then &lt;code&gt;successor&lt;/code&gt; is straightforward:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) until 24 main (argc=1, argv=0x7fffffffdd88) at lambda1.cc:24 24 printf ("Answer 0.0 is %d\n", apply0 (successor, 3)); (gdb) step apply0 (fn=0x401156 &lt;successor(int)&gt;, arg=3) at lambda1.cc:8 8 return fn (arg); (gdb) step successor (i=3) at lambda1.cc:4 4 int successor (int i) { return i + 1; } (gdb) print i $1 = 3 (gdb) backtrace #0 successor (i=3) at lambda1.cc:4 #1 0x000000000040117f in apply0 (fn=0x401156 &lt;successor(int)&gt;, arg=3) at lambda1.cc:8 #2 0x0000000000401305 in main (argc=1, argv=0x7fffffffdd88) at lambda1.cc:24 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The above example shows GDB's &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Continuing-and-Stepping.html#index-until"&gt;&lt;code&gt;until&lt;/code&gt; command&lt;/a&gt;, which, in this case, can be used to advance program execution to the specified line, in this case line 24. (Note: The &lt;code&gt;until&lt;/code&gt; command is used for other purposes too; don't expect it to always advance to the specified line number.)&lt;/p&gt; &lt;p&gt;Next, the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Continuing-and-Stepping.html#index-step"&gt;&lt;code&gt;step&lt;/code&gt; command&lt;/a&gt; is used twice, once to step into &lt;code&gt;apply0&lt;/code&gt; and then again into &lt;code&gt;successor&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;After that, a &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Data.html#index-print"&gt;&lt;code&gt;print&lt;/code&gt; command&lt;/a&gt; is used to show the value of the argument &lt;code&gt;i&lt;/code&gt; which is in the &lt;code&gt;successor&lt;/code&gt; function.&lt;/p&gt; &lt;p&gt;Finally, the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Backtrace.html#index-backtrace"&gt;&lt;code&gt;backtrace&lt;/code&gt; command&lt;/a&gt; is used to show the stack trace.&lt;/p&gt; &lt;p&gt;These commands are straightforward and should be unsurprising to anyone accustomed to using GDB. Ideally, we'd like the debugging of lambda expressions to be just as straightforward, though we'll soon see that this is not the case.&lt;/p&gt; &lt;h2&gt;Function objects: Debugging apply1 and successor&lt;/h2&gt; &lt;p&gt;The first gotcha occurs when using a function object. To demonstrate this, I've defined &lt;code&gt;apply1&lt;/code&gt; to be very similar to &lt;code&gt;apply0&lt;/code&gt;, with the only difference being that instead of taking a function pointer as the first argument (as is done in &lt;code&gt;apply0&lt;/code&gt;), the first argument of &lt;code&gt;apply1&lt;/code&gt; is a function object which takes an int argument and returns an int. As a reminder, this is what &lt;code&gt;apply1&lt;/code&gt; looks like:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt;int apply1 (std::function&lt;int(int)&gt; fn, int arg) { return fn (arg); } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In GDB, we'll first advance to line 25 by first using the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Set-Breaks.html#index-tbreak"&gt;&lt;code&gt;tbreak&lt;/code&gt; command&lt;/a&gt; to place a temporary breakpoint, after which we use the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Continuing-and-Stepping.html#index-c-_0028continue_0029"&gt;&lt;code&gt;continue&lt;/code&gt; command&lt;/a&gt; to advance to that breakpoint:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) tbreak 25 Temporary breakpoint 2 at 0x4012a9: file lambda.cc, line 25. (gdb) continue Continuing. Answer 1 is 4 Temporary breakpoint 2, main (argc=1, argv=0x7fffffffdd98) at lambda.cc:25 25 printf ("Answer 2 is %d\n", apply1 (successor, 4)); /* Line 25 */ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Before attempting to step into &lt;code&gt;apply1&lt;/code&gt;, let's create a checkpoint to which we'll come back later:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) checkpoint checkpoint 1: fork returned pid 4178814. &lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; GDB's &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Checkpoint_002fRestart.html#index-checkpoint-1"&gt;&lt;code&gt;checkpoint&lt;/code&gt; command&lt;/a&gt; has several limitations: it doesn't work for multi-threaded programs and it also only works on GNU/Linux. But when it can be used, it's very handy for situations where you might wish to return to an earlier program state.&lt;/p&gt; &lt;p&gt;Now, let's see what happens when we try to step into &lt;code&gt;apply1&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) step std::function&lt;int (int)&gt;::function&lt;int (&amp;)(int), void&gt;(int (&amp;)(int)) ( this=0x7fffffffdb90, __f=@0x401156: {int (int)} 0x401156 &lt;successor(int)&gt;) at /usr/include/c++/12/bits/std_function.h:437 437 : _Function_base() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;What has happened here is that we've stepped into code which is constructing the function object, which is not what we wanted—we wanted to step into &lt;code&gt;apply1&lt;/code&gt;. To get past this, we can use the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Continuing-and-Stepping.html#index-fin-_0028finish_0029"&gt;&lt;code&gt;finish&lt;/code&gt; command&lt;/a&gt; (which returns us to &lt;code&gt;main&lt;/code&gt; at line 25) and then &lt;code&gt;step&lt;/code&gt; again, which will get us into &lt;code&gt;apply1&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) finish Run till exit from #0 std::function&lt;int (int)&gt;::function&lt;int (&amp;)(int), void&gt;(int (&amp;)(int)) (this=0x7fffffffdb90, __f=@0x401156: {int (int)} 0x401156 &lt;successor(int)&gt;) at /usr/include/c++/12/bits/std_function.h:437 0x00000000004012bd in main (argc=1, argv=0x7fffffffdd88) at lambda.cc:25 25 printf ("Answer 2 is %d\n", apply1 (successor, 4)); /* Line 25 */ (gdb) step apply1(std::function&lt;int (int)&gt;, int) (fn=..., arg=4) at lambda.cc:13 13 return fn (arg);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A more complicated call might require the GDB user to issue multiple &lt;code&gt;finish&lt;/code&gt; / &lt;code&gt;step&lt;/code&gt; commands in order to end up in the desired function.&lt;/p&gt; &lt;h2&gt;Using GDB's skip command to avoid stepping into function object constructors&lt;/h2&gt; &lt;p&gt;Instead of using &lt;code&gt;finish&lt;/code&gt; and then &lt;code&gt;step&lt;/code&gt; as shown above, let's look at a way that GDB can more directly step into apply1. First, we'll return to the checkpoint that we created earlier—this is done by using the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Checkpoint_002fRestart.html#index-restart-checkpoint_002did"&gt;&lt;code&gt;restart&lt;/code&gt; command&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) restart 1 Switching to Thread 0x7ffff7a89400 (LWP 4178814) #0 main (argc=1, argv=0x7fffffffdd98) at lambda.cc:25 25 printf ("Answer 2 is %d\n", apply1 (successor, 4)); /* Line 25 */ (gdb)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now we'll use GDB's &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Skipping-Over-Functions-and-Files.html#index-skip"&gt;&lt;code&gt;skip&lt;/code&gt; command&lt;/a&gt;.  The &lt;code&gt;skip&lt;/code&gt; command shown here will cause GDB to skip, while stepping, any calls to any method in the std::function class, which includes the constructor that we ran into earlier. This particular &lt;code&gt;skip&lt;/code&gt; command uses the &lt;code&gt;-rfunction&lt;/code&gt; option with the regular expression &lt;code&gt;^std::function.*&lt;/code&gt;. The &lt;code&gt;-rfunction&lt;/code&gt; option indicates that the GDB &lt;em&gt;skip&lt;/em&gt; machinery should attempt to match functions at which it might stop against the specified regular expression.&lt;/p&gt; &lt;p&gt;In this case, the regular expression is &lt;code&gt;^std::function.*&lt;/code&gt;. The carat (&lt;code&gt;^&lt;/code&gt;) matches the beginning of the string being matched, followed by the literal &lt;code&gt;std::function&lt;/code&gt;. Finally, the&lt;code&gt;.*&lt;/code&gt; matches the rest of the string. (If you want to know more about regular expressions, I wholeheartedly recommend Jeffrey Friedl's book, &lt;a href="https://www.oreilly.com/library/view/mastering-regular-expressions/0596528124/"&gt;Mastering Regular Expressions&lt;/a&gt;.)&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) skip -rfunction ^std::function.* Function(s) ^std::function.* will be skipped when stepping. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, with the &lt;code&gt;skip&lt;/code&gt; in place, let's try the &lt;code&gt;step&lt;/code&gt; again:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) step apply1(std::function&lt;int (int)&gt;, int) (fn=..., arg=4) at lambda.cc:13 13 return fn (arg); (gdb) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This time, due to the use of the &lt;code&gt;skip&lt;/code&gt; command, shown above, we're able to &lt;code&gt;step&lt;/code&gt; into &lt;code&gt;apply1&lt;/code&gt; in a similar fashion as shown for &lt;code&gt;apply0&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Attempting to step into the function call in apply1&lt;/h2&gt; &lt;p&gt;In attempting to step into the function call on line 13 (shown above), we'll encounter another "gotcha." So let's create another checkpoint:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) checkpoint checkpoint 2: fork returned pid 4193641. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, let's see what happens when we step:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) step 14 }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This didn't step into the successor function as expected.  It turns out that the skip command that we used earlier to avoid seeing the call to the function object constructor is now working against us.  When, while stepping, GDB finds a function to skip, it also skips all functions called by the function in question, even if some descendant call is to a function which wouldn't have been otherwise skipped. To better see what's gone wrong, let's restart at the most recent checkpoint and then disable the skip using the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Skipping-Over-Functions-and-Files.html#index-skip-disable"&gt;&lt;code&gt;skip disable&lt;/code&gt; command&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) restart 2 Switching to Thread 0x7ffff7a89400 (LWP 4193641) #0 apply1(std::function&lt;int (int)&gt;, int) (fn=..., arg=4) at lambda.cc:13 13 return fn (arg); (gdb) info skip Num Enb Glob File RE Function 1 y n &lt;none&gt; y ^std::function.* (gdb) skip disable 1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Also shown above, is the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Skipping-Over-Functions-and-Files.html#index-info-skip"&gt;&lt;code&gt;info skip&lt;/code&gt; command&lt;/a&gt;, which is used to display a list of the things to be skipped while stepping.&lt;/p&gt; &lt;p&gt;Now, with the &lt;code&gt;skip&lt;/code&gt; on methods in &lt;code&gt;std::function&lt;/code&gt; disabled, let's try stepping again:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) step std::function&lt;int (int)&gt;::operator()(int) const (this=0x7fffffffdb90, __args#0=4) at /usr/include/c++/12/bits/std_function.h:589 589 if (_M_empty())&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We've ended up in the implementation of &lt;code&gt;operator()(int)&lt;/code&gt; in &lt;code&gt;std::function&lt;/code&gt;. While it is possible to end up in &lt;code&gt;successor()&lt;/code&gt; after stepping some more, this is tedious. It is better to instead use the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Set-Breaks.html#index-break"&gt;&lt;code&gt;break&lt;/code&gt; command&lt;/a&gt; to set a breakpoint in &lt;code&gt;successor&lt;/code&gt; and continue:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) break successor Breakpoint 3 at 0x40115d: file lambda.cc, line 4. (gdb) continue Continuing. Breakpoint 3, successor (i=4) at lambda.cc:4 4 int successor (int i) { return i + 1; } (gdb) print i $2 = 4 (gdb) backtrace #0 successor (i=4) at lambda.cc:4 #1 0x00000000004026d8 in std::__invoke_impl&lt;int, int (*&amp;)(int), int&gt; ( __f=@0x7fffffffdb90: 0x401156 &lt;successor(int)&gt;) at /usr/include/c++/12/bits/invoke.h:61 #2 0x00000000004025ac in std::__invoke_r&lt;int, int (*&amp;)(int), int&gt; ( __fn=@0x7fffffffdb90: 0x401156 &lt;successor(int)&gt;) at /usr/include/c++/12/bits/invoke.h:114 #3 0x0000000000402452 in std::_Function_handler&lt;int (int), int (*)(int)&gt;::_M_invoke(std::_Any_data const&amp;, int&amp;&amp;) (__functor=..., __args#0=@0x7fffffffdae4: 4) at /usr/include/c++/12/bits/std_function.h:290 #4 0x0000000000402260 in std::function&lt;int (int)&gt;::operator()(int) const ( this=0x7fffffffdb90, __args#0=4) at /usr/include/c++/12/bits/std_function.h:591 #5 0x00000000004011a1 in apply1(std::function&lt;int (int)&gt;, int) (fn=..., arg=4) at lambda.cc:13 #6 0x00000000004012d1 in main (argc=1, argv=0x7fffffffdd88) at lambda.cc:25&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note how much more complicated this backtrace is compared to that of &lt;code&gt;apply0&lt;/code&gt; and &lt;code&gt;successor&lt;/code&gt;. This is due to the fact that frames corresponding to calls to C++ support functions for invoking a function object are still on the stack.&lt;/p&gt; &lt;h2&gt;Stopping in a lambda expression&lt;/h2&gt; &lt;p&gt;Thus far, we've encountered difficulties when stepping into a function to which a function object is passed. That problem is easily avoided by either using &lt;code&gt;finish&lt;/code&gt; followed by (another) &lt;code&gt;step&lt;/code&gt; or by using GDB's &lt;code&gt;skip&lt;/code&gt; command.&lt;/p&gt; &lt;p&gt;We've also seen that stepping into a call of a function object is problematic - instead of stepping directly into the function object, we instead step into some C++ implementation details related to making such a function call. Assuming we know the function being called, we can simply set a breakpoint on it and then continue.&lt;/p&gt; &lt;p&gt;This is the approach that should be taken for debugging a C++ lambda expression: set a breakpoint on it. Due to its anonymous nature, you don't know its name, so, instead, you must set the breakpoint via its line number. But this too, might be somewhat surprising. Let's take a closer look by placing a breakpoint on the lambda expression on line 33. As a reminder, the code looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-cpp"&gt; printf ("Answer 5 is %d\n", apply1 ([n] (int i) { return i + n; }, 4)); /* Line 33 */ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is what happens when we set a breakpoint on line 33:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) break 33 Breakpoint 4 at 0x40124f: /home/kev/examples/lambda.cc:33. (2 locations) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the message indicates that breakpoint 4 has been set at 2 locations. We'll use the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Set-Breaks.html#index-info-breakpoints"&gt;&lt;code&gt;info breakpoints&lt;/code&gt; command&lt;/a&gt; (abbreviated to &lt;code&gt;info break&lt;/code&gt;) to find out more:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) info break 4 Num Type Disp Enb Address What 4 breakpoint keep y &lt;MULTIPLE&gt; 4.1 y 0x000000000040124f in operator()(int) const at lambda.cc:33 4.2 y 0x000000000040136b in main(int, char**) at lambda.cc:33 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This shows the two locations upon which the breakpoint has been set. Breakpoint number 4.2 is in &lt;code&gt;main&lt;/code&gt; and will be the breakpoint that is hit first. The other breakpoint, indicated by 4.1, is the breakpoint for the lambda expression. Let's see what happens when we continue first to 4.2 and then to 4.1, and then look at some program state.  Note that we'll need to &lt;code&gt;continue&lt;/code&gt; twice, stopping once in &lt;code&gt;main&lt;/code&gt;, and the second time in the lambda function.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) continue Continuing. Answer 2 is 5 Answer 3 is 2 Answer 4 is 3 Breakpoint 4.2, main (argc=1, argv=0x7fffffffdd98) at lambda.cc:33 33 apply1 ([n] (int i) { return i + n; }, 4)); /* Line 33 */ (gdb) continue Continuing. Breakpoint 4.1, operator() (__closure=0x7fffffffdc00, i=4) at lambda.cc:33 33 apply1 ([n] (int i) { return i + n; }, 4)); /* Line 33 */ (gdb) print i $2 = 4 (gdb) print n $3 = 7 (gdb) backtrace #0 operator() (__closure=0x7fffffffdc00, i=4) at lambda.cc:33 #1 0x0000000000401ff0 in std::__invoke_impl&lt;int, main(int, char**)::&lt;lambda(int)&gt;&amp;, int&gt;(std::__invoke_other, struct {...} &amp;) (__f=...) at /usr/include/c++/12/bits/invoke.h:61 #2 0x0000000000401d3e in std::__invoke_r&lt;int, main(int, char**)::&lt;lambda(int)&gt;&amp;, int&gt;(struct {...} &amp;) (__fn=...) at /usr/include/c++/12/bits/invoke.h:114 #3 0x0000000000401954 in std::_Function_handler&lt;int(int), main(int, char**)::&lt;lambda(int)&gt; &gt;::_M_invoke(const std::_Any_data &amp;, int &amp;&amp;) (__functor=..., __args#0=@0x7fffffffdaf4: 4) at /usr/include/c++/12/bits/std_function.h:290 #4 0x0000000000402260 in std::function&lt;int (int)&gt;::operator()(int) const ( this=0x7fffffffdc00, __args#0=4) at /usr/include/c++/12/bits/std_function.h:591 #5 0x00000000004011a1 in apply1(std::function&lt;int (int)&gt;, int) (fn=..., arg=4) at lambda.cc:13 #6 0x0000000000401398 in main (argc=1, argv=0x7fffffffdd98) at lambda.cc:32 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that there are four stack frames in between &lt;code&gt;apply1&lt;/code&gt; and the lambda function at frame #0. This is the C++ support code responsible for invoking the lambda expression. Also note that GDB is able print both the argument &lt;code&gt;i&lt;/code&gt; and the captured variable &lt;code&gt;n&lt;/code&gt; which was declared in &lt;code&gt;main&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Stopping in a particular invocation of a lambda expression&lt;/h2&gt; &lt;p&gt;The example program defines a function which returns a function object representing a lambda expression:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;std::function&lt;int(int)&gt; make_function(int&amp; x) { return [&amp;] (int i) { return i + x; }; /* Line 17 */ } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It's used twice as follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; auto lf2 = make_function (n); printf ("Answer 6 is %d\n", apply1 (lf2, 1)); /* Line 36 */ auto lf3 = make_function (m); printf ("Answer 7 is %d\n", apply1 (lf3, -14)); /* Line 39 */ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Suppose that we wish to only stop in the lambda expression invoked on line 39. The lambda expression is actually on line 17, so we'll have to place a breakpoint on that line, but, as we'll see, the code for the lambda expression is also invoked via the call to apply1 on line 36. We'll arrive at the solution in stages.&lt;/p&gt; &lt;p&gt;We can start out by placing a breakpoint on line 17, after first making another checkpoint. (I'm making another checkpoint so that we can easily back up to an earlier execution point without having to start the program from scratch.) Also, we'll take a look at the locations at which GDB has placed breakpoints:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) checkpoint checkpoint 3: fork returned pid 2030341. (gdb) break 17 Breakpoint 5 at 0x4011af: /home/kev/examples/lambda.cc:17. (2 locations) (gdb) info breakpoint 5 Num Type Disp Enb Address What 5 breakpoint keep y &lt;MULTIPLE&gt; 5.1 y 0x00000000004011af in operator()(int) const at lambda.cc:17 5.2 y 0x00000000004011cf in make_function(int&amp;) at lambda.cc:17 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;info breakpoint 5&lt;/code&gt; command shows that breakpoint 5.2 is in &lt;code&gt;make_function&lt;/code&gt; and that breakpoint 5.1 is in &lt;code&gt;operator()(int)&lt;/code&gt;, which is the lambda expression. Our goal is to stop only in the lambda expression, so let's disable breakpoint 5.2.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) disable 5.2 (gdb) info breakpoint 5 Num Type Disp Enb Address What 5 breakpoint keep y &lt;MULTIPLE&gt; 5.1 y 0x00000000004011af in operator()(int) const at lambda.cc:17 5.2 n 0x00000000004011cf in make_function(int&amp;) at lambda.cc:17 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now let's see what happens when we &lt;code&gt;continue&lt;/code&gt;.  It'll take two &lt;code&gt;continue&lt;/code&gt; commands to get to the lambda invoked by the &lt;code&gt;apply1&lt;/code&gt; call at line 39:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) continue Continuing. Answer 5 is 11 Breakpoint 5.1, operator() (__closure=0x7fffffffdc20, i=1) at lambda.cc:17 17 return [&amp;] (int i) { return i + x; }; /* Line 17 */ (gdb) continue Continuing. Answer 6 is 8 Breakpoint 5.1, operator() (__closure=0x7fffffffdc40, i=-14) at lambda.cc:17 17 return [&amp;] (int i) { return i + x; }; /* Line 17 */ (gdb) backtrace #0 operator() (__closure=0x7fffffffdc40, i=-14) at lambda.cc:17 #1 0x0000000000401e70 in std::__invoke_impl&lt;int, make_function(int&amp;)::&lt;lambda(int)&gt;&amp;, int&gt;(std::__invoke_other, struct {...} &amp;) (__f=...) at /usr/include/c++/12/bits/invoke.h:61 #2 0x0000000000401a79 in std::__invoke_r&lt;int, make_function(int&amp;)::&lt;lambda(int)&gt;&amp;, int&gt;(struct {...} &amp;) (__fn=...) at /usr/include/c++/12/bits/invoke.h:114 #3 0x000000000040174e in std::_Function_handler&lt;int(int), make_function(int&amp;)::&lt;lambda(int)&gt; &gt;::_M_invoke(const std::_Any_data &amp;, int &amp;&amp;) (__functor=..., __args#0=@0x7fffffffdae4: -14) at /usr/include/c++/12/bits/std_function.h:290 #4 0x0000000000402260 in std::function&lt;int (int)&gt;::operator()(int) const ( this=0x7fffffffdc40, __args#0=-14) at /usr/include/c++/12/bits/std_function.h:591 #5 0x00000000004011a1 in apply1(std::function&lt;int (int)&gt;, int) (fn=..., arg=-14) at lambda.cc:13 #6 0x0000000000401452 in main (argc=1, argv=0x7fffffffdd88) at lambda.cc:39&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;backtrace&lt;/code&gt; shows that the second &lt;code&gt;continue&lt;/code&gt; command brought us to the lambda expression invoked by the the call to &lt;code&gt;apply1&lt;/code&gt; at line 39 in &lt;code&gt;main&lt;/code&gt;. For this simple program, it's not onerous to continue twice, but in real application code, it might happen that hundreds or thousands of &lt;code&gt;continue&lt;/code&gt; commands might be needed to stop at the point of interest.&lt;/p&gt; &lt;p&gt;To show how we can stop at a breakpoint which might be hit after some other line of code has been executed, let's first restart from checkpoint 3:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) restart 3 Switching to Thread 0x7ffff7a89400 (LWP 2030341) #0 operator() (__closure=0x7fffffffdbf0, i=4) at lambda.cc:33 33 apply1 ([n] (int i) { return i + n; }, 4)); /* Line 33 */&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This causes GDB to switch to the fork created in the previous checkpoint, but it doesn't undo any of the breakpoints that we set up after creating the checkpoint. To see this, let's look again at breakpoint 5:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) info breakpoint 5 Num Type Disp Enb Address What 5 breakpoint keep y &lt;MULTIPLE&gt; breakpoint already hit 2 times 5.1 y 0x00000000004011af in operator()(int) const at lambda.cc:17 5.2 n 0x00000000004011cf in make_function(int&amp;) at lambda.cc:17 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is pretty close to the state of breakpoint 5 when we last looked at it. One difference is that it tells us that breakpoint 5 has (overall) been hit 2 times, whereas that message was missing we we looked at it earlier.&lt;/p&gt; &lt;p&gt;Now, just so it's clear where we are in the program, we'll set a temporary breakpoint at line 35 and then continue:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) tbreak 35 Temporary breakpoint 6 at 0x4013b5: file lambda.cc, line 35. (gdb) continue Continuing. Answer 5 is 11 Temporary breakpoint 6, main (argc=1, argv=0x7fffffffdd88) at lambda.cc:35 35 auto lf2 = make_function (n);&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GDB has stopped on the first call to &lt;code&gt;make_function&lt;/code&gt;. The following line, 36, will call &lt;code&gt;apply1&lt;/code&gt; using the function object held in &lt;code&gt;lf2&lt;/code&gt;. Recall that it's our goal to stop in the lambda expression invoked by calling &lt;code&gt;apply1&lt;/code&gt; on line 39. In order to realize that goal, let's first disable breakpoint 5 and then look at what &lt;code&gt;info breakpoint&lt;/code&gt; says about it:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) disable 5 (gdb) info break 5 Num Type Disp Enb Address What 5 breakpoint keep n &lt;MULTIPLE&gt; breakpoint already hit 2 times 5.1 y- 0x00000000004011af in operator()(int) const at lambda.cc:17 5.2 n 0x00000000004011cf in make_function(int&amp;) at lambda.cc:17&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Examining the 'Enb' column shows that, overall, breakpoint 5 is disabled, but were it to be enabled, then breakpoint 5.1 would also be enabled, while breakpoint 5.2 would still be disabled. If we were to continue at this point, the program would not stop at either of the breakpoint 5 locations since it is currently disabled.&lt;/p&gt; &lt;p&gt;The next step is to place a breakpoint at line 39 and then add some commands to run when that breakpoint is hit.  Specifically, we'll enable breakpoint 5 as one of the commands. The &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb.html/Break-Commands.html#index-commands"&gt;&lt;code&gt;commands&lt;/code&gt; command&lt;/a&gt; is used to associate some GDB commands with a breakpoint; the &lt;code&gt;commands&lt;/code&gt; command can be given an argument specifying the breakpoint number (to which to associate some GDB commands), but when no breakpoint number is specified, as is shown below, it simply attaches commands to the most recently created breakpoint.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) break 39 Breakpoint 7 at 0x40142b: file lambda.cc, line 39. (gdb) commands Type commands for breakpoint(s) 7, one per line. End with a line saying just "end". &gt;enable 5 &gt;continue &gt;end&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After enabling breakpoint 5 (via the command &lt;code&gt;enable 5&lt;/code&gt;), a &lt;code&gt;continue&lt;/code&gt; command will be issued. Thus, this breakpoint won't stop for user interaction, but will continue execution after first enabling breakpoint #5, which is for the lambda expression that we want to stop in. This is what happens when we continue:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) continue Continuing. Answer 6 is 8 Breakpoint 7, main (argc=1, argv=0x7fffffffdd88) at lambda.cc:39 39 printf ("Answer 7 is %d\n", apply1 (lf3, -14)); /* Line 39 */ Breakpoint 5.1, operator() (__closure=0x7fffffffdc40, i=-14) at lambda.cc:17 17 return [&amp;] (int i) { return i + x; }; /* Line 17 */ (gdb) print x $5 = (int &amp;) @0x7fffffffdb88: -28 (gdb) print i $6 = -14 (gdb) backtrace #0 operator() (__closure=0x7fffffffdc40, i=-14) at lambda.cc:17 #1 0x0000000000401e70 in std::__invoke_impl&lt;int, make_function(int&amp;)::&lt;lambda(int)&gt;&amp;, int&gt;(std::__invoke_other, struct {...} &amp;) (__f=...) at /usr/include/c++/12/bits/invoke.h:61 #2 0x0000000000401a79 in std::__invoke_r&lt;int, make_function(int&amp;)::&lt;lambda(int)&gt;&amp;, int&gt;(struct {...} &amp;) (__fn=...) at /usr/include/c++/12/bits/invoke.h:114 #3 0x000000000040174e in std::_Function_handler&lt;int(int), make_function(int&amp;)::&lt;lambda(int)&gt; &gt;::_M_invoke(const std::_Any_data &amp;, int &amp;&amp;) (__functor=..., __args#0=@0x7fffffffdae4: -14) at /usr/include/c++/12/bits/std_function.h:290 #4 0x0000000000402260 in std::function&lt;int (int)&gt;::operator()(int) const ( this=0x7fffffffdc40, __args#0=-14) at /usr/include/c++/12/bits/std_function.h:591 #5 0x00000000004011a1 in apply1(std::function&lt;int (int)&gt;, int) (fn=..., arg=-14) at lambda.cc:13 #6 0x0000000000401452 in main (argc=1, argv=0x7fffffffdd88) at lambda.cc:39&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that GDB shows that breakpoint 7 was hit, but it doesn't stop at it. If we wanted to cause that message to not be printed, we could have used the special command &lt;code&gt;silent&lt;/code&gt; as the first command in the commands for breakpoint #7. (Note: the &lt;code&gt;silent&lt;/code&gt; command can only be used as a command within the &lt;code&gt;commands&lt;/code&gt; command.)&lt;/p&gt; &lt;p&gt;The above example also prints the value of the captured variable &lt;code&gt;x&lt;/code&gt; and the argument (to the lambda function) &lt;code&gt;i&lt;/code&gt;. Frame #6 of the backtrace shows that the call to &lt;code&gt;apply1&lt;/code&gt; was invoked from line 39 in function &lt;code&gt;main&lt;/code&gt;. Frames #0, #5, and #6 correspond to lines of code in our example program. The remaining frames, #1 thru #4, show calls to functions within the C++ library.&lt;/p&gt; &lt;p&gt;GDB's ability to execute commands associated with a breakpoint is a powerful feature that's useful in many other situations as well.&lt;/p&gt; &lt;h2&gt;&lt;a id="Debugging an LLVM compiled program" name="Debugging an LLVM compiled program"&gt;&lt;/a&gt;Debugging an LLVM-compiled program&lt;/h2&gt; &lt;p&gt;As noted earlier, the interactions with GDB shown above were performed against an executable built with GCC 12.1.&lt;/p&gt; &lt;p&gt;If you're using clang++ instead of the GNU compiler, most of the interactions will be similar if not identical to that shown above. One somewhat important difference is the order in which the locations for a breakpoint on a line containing a lambda expression are shown. When using the LLVM compiler, these locations are reversed from that shown for the GNU compiler. E.g., when setting a breakpoint on line 33, we (might) see this instead when debugging an executable produced by the LLVM compiler:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) break 33 Breakpoint 4 at 0x4013bb: /home/kev/examples/lambda.cc:33. (2 locations) (gdb) info break 4 Num Type Disp Enb Address What 4 breakpoint keep y &lt;MULTIPLE&gt; 4.1 y 0x00000000004013bb in main(int, char**) at lambda.cc:33 4.2 y 0x000000000040205f in main::$_2::operator()(int) const at lambda.cc:33&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This isn't especially important for breakpoint 4, but it is important for breakpoint 5 which was placed on line 17. Before, when using the GCC executable, we disabled the breakpoint on 5.2 in order to disable the breakpoint on the outer scope. But, when using an LLVM executable, we instead have to disable the breakpoint on 5.1 instead. I.e.:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;(gdb) disable 5.1 (gdb) info breakpoint 5 Num Type Disp Enb Address What 5 breakpoint keep y &lt;MULTIPLE&gt; 5.1 n 0x00000000004011f7 in make_function(int&amp;) at lambda.cc:17 5.2 y 0x000000000040195f in make_function(int&amp;)::$_0::operator()(int) const at lambda.cc:17&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This could also happen with some past or future version of the GNU compiler. You shouldn't assume that you know the order of the locations, but should instead use the &lt;code&gt;info breakpoints&lt;/code&gt; command to figure out which breakpoint location to disable or even delete.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;This article has discussed several problems that might encountered when using GDB to debug function objects. These problems include inadvertently stepping into constructors as well as the related problem of stepping into function object invocation code. In order to avoid the latter problem, it is best to place a breakpoint in the target function. This can be done by name, assuming we know the name, but in the case of lambda expressions, there is no name, so the breakpoint must be placed via line number.&lt;/p&gt; &lt;p&gt;When placing a breakpoint on a line containing a lambda, it's frequently the case that the breakpoint will be placed at multiple locations. The &lt;code&gt;info breakpoints&lt;/code&gt; command can be used to determine which location is that of the containing function and which is the lambda; once this is determined, GDB's &lt;code&gt;disable&lt;/code&gt; command can be used to cause GDB to not stop at one of those locations.&lt;/p&gt; &lt;p&gt;Finally, a helper breakpoint with associated commands can be used to re-enable a breakpoint that's been disabled. The combination of first disabling a breakpoint on a lambda and then using a helper breakpoint to re-enable the lambda's breakpoint is useful for targeting a specific invocation of a lambda expression or other function object.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/03/how-debug-c-lambda-expressions-gdb" title="How to debug C++ lambda expressions with GDB"&gt;How to debug C++ lambda expressions with GDB&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Kevin Buettner</dc:creator><dc:date>2023-05-03T07:00:00Z</dc:date></entry><entry><title>How to set up event-driven microservices using Knative Eventing</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/02/how-set-event-driven-microservices-using-knative-eventing" /><author><name>Matthias Wessendorf</name></author><id>0244c1a3-a6bf-4876-9c57-26b9ab99e820</id><updated>2023-05-02T07:00:00Z</updated><published>2023-05-02T07:00:00Z</published><summary type="html">&lt;p&gt;Many modern application designs are &lt;a href="https://developers.redhat.com/topics/event-driven/"&gt;event-driven&lt;/a&gt;, aiming to deliver events quickly. This article describes how to orchestrate event-driven &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservices&lt;/a&gt; using standards like CNCF CloudEvents and &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; APIs for Knative to simplify EDA-style application development.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://www.redhat.com/en/topics/integration/what-is-event-driven-architecture"&gt;Event Driven Architecture (EDA)&lt;/a&gt; allows the implementation of loosely coupled applications and services. In this model, event producers do not know for which event consumers are listening, and the event itself does not know the consequences of its occurrence. EDA is a good option for distributed application architectures.&lt;/p&gt; &lt;p&gt;Figure 1 illustrates an example of an event-driven application consisting of three services, producing and consuming different events with an event bus responsible for the orchestration and routing of the events. Note that “Service 3” produces an event indicating the business process finished, but there is no consumer for the event.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-02-27_18-13-39.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-02-27_18-13-39.png?itok=Z1ss7cix" width="600" height="515" alt="Illustration of an event-driven application consisting of three services, producing and consuming different events." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: An event-driven application consisting of three services, producing and consuming different events.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;It is simple to implement another consumer for the “Finish Event” with the flexible architecture of event-driven systems.&lt;/p&gt; &lt;h2&gt;Knative Eventing&lt;/h2&gt; &lt;p&gt;How can you build a system on Kubernetes that orchestrates events and routes them to consumers? Luckily there is &lt;a href="https://knative.dev/docs/eventing"&gt;Knative Eventing&lt;/a&gt;, which offers a collection of APIs that enable cloud native developers to use an event-driven architecture within their applications and services. You can use these APIs to create components that route events from event producers to event consumers, known as sinks, that receive events.&lt;/p&gt; &lt;p&gt;Knative Eventing uses standard HTTP requests to send and receive events between event producers and sinks. These events conform to a CNCF industry standard called &lt;a href="https://cloudevents.io/"&gt;CloudEvents&lt;/a&gt;, which enables creating, parsing, sending, and receiving events in any programming language. The binding between the HTTP protocol and CloudEvents is standardized in this &lt;a href="https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/bindings/http-protocol-binding.md"&gt;specification&lt;/a&gt;. Although the focus of this article is on HTTP, it is worth mentioning that the CloudEvents specification also describes bindings for &lt;a href="https://github.com/cloudevents/spec/tree/v1.0.2/cloudevents/bindings"&gt;other protocols&lt;/a&gt;, such as &lt;a href="http://docs.oasis-open.org/amqp/core/v1.0/os/amqp-core-overview-v1.0-os.html"&gt;AMQP&lt;/a&gt; or &lt;a href="https://datatracker.ietf.org/doc/html/rfc6455"&gt;Websocket&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Event mesh with Knative Broker&lt;/h2&gt; &lt;p&gt;One of the key APIs in Knative Eventing is the &lt;a href="https://knative.dev/docs/eventing/brokers"&gt;Knative Broker API&lt;/a&gt;, which defines an &lt;a href="https://knative.dev/docs/eventing/event-mesh/"&gt;event mesh&lt;/a&gt; aiding the event orchestration and routing. Figure 2 shows a complete process, covering purchases from an online web shop and ends when the order is completely delivered at the customer's door. The process is implemented by a couple of microservice applications that are consuming and producing events. There is no direct communication or invocation between the services. Instead, the applications are loosely coupled and communicate only via events.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screenshot%20from%202023-03.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Screenshot%20from%202023-03.png?itok=Uj_tWe6i" width="600" height="272" alt="An illustration of the process flow of an event-driven application." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The complete process flow of an event-driven application.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The orchestration of the event exchange is handled by an event mesh. In our case, this is the Knative Broker for &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: eventing.knative.dev/v1 kind: Broker metadata:  annotations:    eventing.knative.dev/broker.class: Kafka  name: order-broker spec:  config:    apiVersion: v1    kind: ConfigMap    name: order-broker-config --- apiVersion: v1 kind: ConfigMap metadata:  name: order-broker-config data:  bootstrap.servers: &lt;url&gt;  auth.secret.ref.name: &lt;optional-secret-name&gt;  default.topic.partitions: "10"  default.topic.replication.factor: "3"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The broker is annotated to pick the Kafka-backed implementation and points to a ConfigMap holding configuration about the Apache Kafka Topic, internally used by the broker. Generally, it is also recommended to configure aspects of delivery guarantees and retries. But we skipped this to keep this article simple. For more details on best practices for Knative Broker configurations, read the article, &lt;a href="https://developers.redhat.com/articles/2023/03/08/configuring-knative-broker-apache-kafka"&gt;Our advice for configuring Knative Broker for Apache Kafka&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;After the Broker definition has been applied with &lt;code&gt;oc apply&lt;/code&gt;, you can check for the broker and its status as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get brokers.eventing.knative.dev -n orders NAME           URL                                                                                 AGE   READY order-broker   http://kafka-broker-ingress.knative-eventing.svc.cluster.local/orders/order-broker   46m   True&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Every Knative Broker object exposes an HTTP endpoint acting as the Ingress for CloudEvents. The URL can be found on the status of each broker object. The following is an example of an HTTP POST request that could be sent from the “Online Shop” service to the event mesh, aka the Knative Kafka Broker.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;curl -v -X POST \ -H "content-type: application/json" \ -H "ce-specversion: 1.0" \ -H "ce-source: /online/shop" \ -H "ce-type: order.requested" \ -H "ce-id: 1f1380d4-8ff2-4ab0-b2ba-54811226c21b" \ -d '{"customerId": 20207-19, "orderId": "f8bc3445-b844"}' \ http://kafka-broker-ingress.knative-eventing.svc.cluster.local/orders/order-broker&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;But how is the event getting delivered to the “Payment Service”, since there is no direct coupling between the two?&lt;/p&gt; &lt;h2&gt;Event orchestration and routing&lt;/h2&gt; &lt;p&gt;While the Broker API implements an event mesh, it goes hand-in-hand with the trigger API, which the broker is using to route messages based on a given set of rules or criteria to their destination.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: trigger-order-requested spec: broker: order-broker filter: attributes: type: order.requested source: /online/shop subscriber: ref: apiVersion:v1 kind: Service name: payment&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This example is a trigger for the “order-broker” which contains two filters, each for different CloudEvent attributes metadata:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Type&lt;/li&gt; &lt;li aria-level="1"&gt;Source&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Both rules are treated as an AND, and the event is only routed to the referenced payment service by the Knative Broker if both rules match.&lt;/p&gt; &lt;p&gt;The routing of the matching CloudEvent to the referenced Kubernetes Service (payment) is done by the Knative Broker using HTTP. This allows a flexible architecture for the implementation of the processing services since simply all that is needed is a Web Server program, regardless of the written language. Besides the Kubernetes Service API, we can also reference a &lt;a href="https://knative.dev/docs/serving/"&gt;Knative Serving Service&lt;/a&gt;, supporting serverless principals.&lt;/p&gt; &lt;p&gt;If the referenced service replies with a CloudEvent in its HTTP response, this event is returned back to the Knative Broker and available for further processing. Using a different trigger with a matching rule can route those events to other service applications.&lt;/p&gt; &lt;h2&gt;CloudEvent processing with Knative Functions&lt;/h2&gt; &lt;p&gt;One simple way to create microservices that are processing standard CloudEvents is to leverage the &lt;a href="https://knative.dev/docs/functions/"&gt;Knative Functions&lt;/a&gt; project. It contains templates for a number of languages and platforms, such as:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/topics/go"&gt;Golang&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; (&lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt;)&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt; (&lt;a href="https://developers.redhat.com/node/219015"&gt;Java&lt;/a&gt;)&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/topics/spring-boot/"&gt;Spring Boot&lt;/a&gt; (Java)&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/topics/rust"&gt;Rust&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The following script is the implementation of the “Payment Service” application, which was written based on the &lt;a href="https://quarkus.io/guides/funqy"&gt;Quarkus Funqy&lt;/a&gt; template. Funqy is part of Quarkus’s serverless strategy that provides a portable Java API for developers to write serverless functions and deploy them to heterogeneous serverless runtimes, including AWS Lambda, Azure Functions, Google Cloud, and Knative. With Funqy, developers can easily bind their methods to CloudEvents, using the @Funq annotation. Funqy ensures that the @Funq annotated Java method is invoked with the HTTP request from the Knative Broker, containing the “OrderRequested” CloudEvent.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The OrderRequests object is part of the CloudEvent payload in a serialized JSON format, as indicated by the previous cURL example.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;@Funq public CloudEvent&lt;PaymentReceived&gt; orderRequest(final CloudEvent&lt;OrderRequested&gt; order) { LOG.debug("Incoming CloudEvent with ID: " + order.id()); try { final PaymentReceived payment = paymentProvider.processPayment(order.data()); return CloudEventBuilder.create() .id(UUID.randomUUID().toString()) .type("payment.received") .source("/payment") .build(payment); } catch (InvalidPaymentException ipe) { // recover from here return CloudEventBuilder.create().build(...); } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The CloudEvent payload is deserialized to the “OrderRequest” type, using the &lt;code&gt;data()&lt;/code&gt; method from the CloudEvents API and processed by a payment provider service. Once the payment is approved, the Knative Function code returns a different CloudEvent with type &lt;strong&gt;payment.received&lt;/strong&gt;, indicating the payment has been received.&lt;/p&gt; &lt;p&gt;Let’s have a look at the diagram in Figure 2 where the “Order Service” subscribed to the “payment.received” event. Whenever such an event is available in the event mesh, the Knative Broker will dispatch it to the subscribed “Order Service” and continue the process of the shopping cart application.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; In case of a failure, we see the &lt;strong&gt;InvalidPaymentException&lt;/strong&gt; and a different CloudEvent with an error type returned to the broker, indicating that a failure has occurred. For more information on how to configure the Knative Broker for delivery guarantees and retries, please refer to the previously mentioned &lt;a href="https://developers.redhat.com/articles/2023/03/08/configuring-knative-broker-apache-kafka"&gt;article&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Using application-specific events&lt;/h3&gt; &lt;p&gt;When working with event-driven microservices, it is highly recommended that every service or function should respond to incoming requests with an outgoing event on its HTTP response. The CloudEvents should be domain-specific and provide context about their state on the CloudEvent metadata attributes. It is very important to not return the same CloudEvent type that goes into a function because this would cause a filter loop on the executing Knative Broker. For successful event processing and domain-specific failures, a service should always return a CloudEvent to the Knative Broker.&lt;/p&gt; &lt;p&gt;To handle network-level failures occurring while the Knative Broker tries to deliver the CloudEvents (such as HTTP 4xx/5xx errors), we recommend configuring a Dead-Letter-Sink for improved delivery guarantees.&lt;/p&gt; &lt;h2&gt;Knative Eventing simplifies event-driven microservices&lt;/h2&gt; &lt;p&gt;The article described how the Knative Eventing Broker and Trigger APIs help to orchestrate event-driven microservices. The architecture is based on standardized Kubernetes APIs for Knative and CNCF CloudEvents. We also discussed how the implementation of EDA-style applications are loosely coupled and how to implement a simple routing approach for events using Knative Eventing. Leveraging industry standards is a good investment for any application architecture. If you have questions, please comment below. We welcome your feedback.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/02/how-set-event-driven-microservices-using-knative-eventing" title="How to set up event-driven microservices using Knative Eventing"&gt;How to set up event-driven microservices using Knative Eventing&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Matthias Wessendorf</dc:creator><dc:date>2023-05-02T07:00:00Z</dc:date></entry><entry><title type="html">New Keycloak maintainer: Sebastian Schuster</title><link rel="alternate" href="https://www.keycloak.org/2023/05/maintainer-sschu" /><author><name>Stian Thorgersen</name></author><id>https://www.keycloak.org/2023/05/maintainer-sschu</id><updated>2023-05-02T00:00:00Z</updated><content type="html">We are pleased to welcome as an official maintainer of Keycloak. Sebastian has contributed to Keycloak since 2019, when he convinced his company Bosch to use Keycloak for identity and access management. He has been active in the community providing help, taking part in discussions and contributing. Behind him, there is a whole team at Bosch providing more than 60 contributions over the last years in various areas. The declarative user profile was the most prominent feature contributed. His company allows him to dedicate a considerable amount of time for Keycloak to help review contributions and reports and get involved in discussions. Since Sebastian has got experience operating Keycloak on a wide scale over several years, he will focus on topics around cloud-native and Keycloak operations like observability. Not only will Sebastian on his own bring a lot of value to Keycloak, but he will also serve as an integration point for Bosch to enable more contributions from his team, allowing them to contribute more value to Keycloak in the future.</content><dc:creator>Stian Thorgersen</dc:creator></entry><entry><title>Why service mesh and API management are better together</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/01/why-service-mesh-and-api-management-are-better-together" /><author><name>Vamsi Ravula</name></author><id>2319ff1a-48e5-4872-82b7-a2bf8e4b32dc</id><updated>2023-05-01T07:00:00Z</updated><published>2023-05-01T07:00:00Z</published><summary type="html">&lt;p&gt;When it comes to managing &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservices&lt;/a&gt;, the possibilities are truly endless with &lt;a href="https://developers.redhat.com/topics/service-mesh/"&gt;service mesh&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/api-management/"&gt;API management&lt;/a&gt;. You might think service mesh and API management are competing technologies—however, they are actually complementary and can help revolutionize the way you manage and secure your microservices and APIs.&lt;/p&gt; &lt;p&gt;In this article, we will explore the benefits of using service mesh and API management together, using a fictitious travel company as an example. &lt;/p&gt; &lt;h2&gt;Service mesh versus API management&lt;/h2&gt; &lt;p&gt;Service mesh is a dedicated infrastructure layer that gives you the power to manage and control communication between microservices, with features like traffic management, service discovery, load balancing, and security.&lt;/p&gt; &lt;p&gt;API management refers to the process for creating, publishing, and managing APIs that connect applications and data across the enterprise and across clouds. This approach lets you control access to your APIs with authentication, authorization, rate limiting, and monitoring.&lt;/p&gt; &lt;p&gt;Together, service mesh and API management can improve the reliability, scalability, security, and performance of your microservices and APIs, as we'll see in the following scenarios.&lt;/p&gt; &lt;h2&gt;Example scenario&lt;/h2&gt; &lt;p&gt;We will use a fictitious travel company, Travelz, to demonstrate the implementation of both service mesh and API management in different scenarios. The subsequent sections will highlight the company's challenges and goals and demonstrate how we can combine these technologies to achieve those goals.&lt;/p&gt; &lt;h3&gt;About the Travelz application&lt;/h3&gt; &lt;p&gt;The travel booking system has two business units that are responsible for different aspects of the travel experience. First is the travel portal business unit that offers multiple travel shops where you can easily search and book your travel.&lt;/p&gt; &lt;p&gt;The second business unit is the travel agency that hosts a set of services such as flights, cars, hotels, etc., designed to provide the travel quotes. The travel portal business unit makes calls to the travel agency services and APIs to provide the quotes.&lt;/p&gt; &lt;h3&gt;Observability, traffic monitoring, and security&lt;/h3&gt; &lt;p&gt;With the growing number of microservices, the team needed a way to manage, secure, and control how the services talk to each other. To connect, manage, and observe the microservices, and to control traffic between services, Travelz decided to adopt Istio-based &lt;a href="https://developers.redhat.com/topics/service-mesh"&gt;Red Hat OpenShift Service Mesh&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;With OpenShift Service Mesh, the team was able to provide the necessary service-to-service capabilities—traffic monitoring, access control, discovery, security, resiliency, metrics, and more—without requiring changes to the code of any of the app’s microservices. The architecture is shown in Figure 1.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/unnamed-4.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/unnamed-4.png?itok=7kPVYUh-" width="600" height="286" alt="A diagram of access via portals versus agency." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Travel agency microservices are managed and monitored using Red Hat OpenShift Service Mesh.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Traffic routing and securing the APIs for external consumption&lt;/h3&gt; &lt;p&gt;Travelz tourism becomes super popular, and now other travel portals want to partner with Travelz. The team aims to enforce these principles in their technology:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Allow secure access to internal services to external partners and clients as APIs.&lt;/li&gt; &lt;li aria-level="1"&gt;APIs should be easy to find, understand, integrate with and adopt.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Travelz builds a new version of their travels service (v2), so their travel partners will access v2 while the internal platforms access v1.&lt;/p&gt; &lt;p&gt;Service mesh's intelligent traffic routing capabilities make it extremely easy to divert the traffic based on who is making the call (internal or external). Travelz IT leveraged service mesh's virtual service capability to achieve this. A &lt;strong&gt;virtual service&lt;/strong&gt; defines a set of traffic routing rules and each routing rule defines matching criteria for the traffic. If the traffic is matched, then it is sent to a named destination service (or subset/version of it). Travelz IT built two different dedicated virtual services: one for external partners and the other for internal portals.&lt;/p&gt; &lt;h3&gt;Managing external access&lt;/h3&gt; &lt;p&gt;To manage access by the external partners, Travelz introduces the &lt;a href="https://developers.redhat.com/products/3scale/overview"&gt;Red Hat 3scale API Management &lt;/a&gt;platform. They adopt a contract-first approach by creating OpenAPI specifications for their existing and new services before onboarding external clients. With the help of the &lt;a href="https://developers.redhat.com/products/3scale/overview"&gt;Red Hat 3scale API Management platform&lt;/a&gt;, Travelz IT now manages partner access to their APIs in such a way that the partners can only access APIs that are protected by a &lt;code&gt;user_key&lt;/code&gt; (see Figure 2). 3scale provides a developer portal out of the box for partner developers to discover, learn, test, and sign up for those APIs.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/unnamed-3.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/unnamed-3.png?itok=zvs83zEB" width="600" height="334" alt="Diagram showing the company manages partner access to their APIs using 3scale." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The company can now securely manage partner access to their APIs using Red Hat 3scale API Management platform.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Before we jump into the next scenario, let’s talk about the concept of domain boundaries. This will help you understand and appreciate the next scenario better.&lt;/p&gt; &lt;h3&gt;Domain boundaries&lt;/h3&gt; &lt;p&gt;As we saw in the earlier two scenarios, traffic direction provides a straightforward guide for when to choose API management or service mesh solutions. It is almost tempting to say you should always choose API management for north-south (external) traffic and service mesh for east-west (internal) traffic.&lt;/p&gt; &lt;p&gt;However, most organizations are not so simple. Typical organizations contain multiple groups that create and manage their own services and interact with other teams and external parties. &lt;strong&gt;Domain boundaries &lt;/strong&gt;can help you divide your organization into smaller, more manageable areas. Much as your enterprise boundary denotes the perimeter of your overall organization, domain boundaries designate the perimeters of groups within your organization. See Figure 3.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/unnamed.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/unnamed.png?itok=LByQC6KL" width="600" height="516" alt="Diagram showing enterprise boundaries for a group of applications." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Using domain boundaries within your organization.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The same north-south and east-west traffic patterns that occur in relation to your enterprise boundary also apply to domain boundaries within your organization. As a result, you should generally choose API management for interdomain traffic and service meshes for intradomain traffic.&lt;/p&gt; &lt;p&gt;Figure 4 will help you better understand the nuances of interdomain versus intradomain traffic patterns.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/unnamed%20%281%29.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/unnamed%20%281%29.png?itok=CsDbBdMN" width="600" height="230" alt="Diagram of interdomain traffic versus intradomain traffic patterns." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: Interdomain versus intradomain traffic patterns.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Interdomain traffic&lt;/strong&gt; crosses domain or enterprise boundaries to connect services with consumers beyond your group or team. Interdomain traffic follows north-south traffic patterns.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Hierarchical 1:N connection structures&lt;/li&gt; &lt;li&gt;Separate service providers and service consumers&lt;/li&gt; &lt;li&gt;Requires authorization and authentication&lt;/li&gt; &lt;li&gt;Formal use contracts needed&lt;/li&gt; &lt;li&gt;Guided service discovery, accessible developer portal, and formal documentation&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;&lt;strong&gt;Intradomain traffic &lt;/strong&gt;stays within domain and enterprise boundaries to link individual microservices. Intradomain traffic typically follows east-west traffic patterns.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Non-hierarchical 1:1 connection structures&lt;/li&gt; &lt;li&gt;Service providers and consumers within the same team&lt;/li&gt; &lt;li&gt;Authentication required&lt;/li&gt; &lt;li&gt;Implicit or informal contracts, if any&lt;/li&gt; &lt;li&gt;Internal documentation within code&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;This is a great e-book if you are interested in more detailed information on this topic: &lt;a href="https://www.redhat.com/en/resources/service-mesh-and-api-management-e-book"&gt;Service mesh or API management? &lt;/a&gt;&lt;/p&gt; &lt;h3&gt;Standardizing API access for internal consumption and collaboration&lt;/h3&gt; &lt;p&gt;With the above context, let’s see how that applies in our example. Travelz IT wants to standardize access of their core agency services for the internal platforms (think interdomain traffic), similar to how they did for external partners. Because all the services are already a part of Red Hat OpenShift Service Mesh, they leverage the seamless integration between &lt;a href="https://developers.redhat.com/topics/service-mesh"&gt;OpenShift Service Mesh&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/3scale/overview"&gt;Red Hat 3scale API management&lt;/a&gt; for this use case.&lt;/p&gt; &lt;p&gt;The WebAssembly extension that enables this is deployed as a sidecar to the microservices, and it communicates directly with the 3scale API manager. It eases the integration of OpenShift Service Mesh and 3scale, and authorizes HTTP requests made to 3scale. It provides a standard way to inject 3scale API Management configurations into OpenShift Service Mesh for execution in a single data plane. In this scenario, service mesh serves as the data plane and 3scale serves as the control plane, eliminating the need for an additional gateway and reducing latency due to the reduced number of hops. See Figure 5.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/unnamed-3_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/unnamed-3_0.png?itok=ajyowzGR" width="600" height="334" alt="Diagram of 3scale and service mesh integration to secure internal traffic." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Simplified integration between 3scale and OpenShift Service Mesh to secure internal traffic. &lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Try this architecture on your own&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/red-hat-architecture-and-design-patterns"&gt;Red Hat solution patterns&lt;/a&gt; provide a solid starting point for those seeking a specific reference architecture to replicate or use as inspiration for technical decision making. These patterns offer guidance on designing, developing, integrating, automating, and delivering cloud-native applications.&lt;/p&gt; &lt;p&gt;The Travelz example is one &lt;a href="https://redhat-solution-patterns.github.io/solution-pattern-apim-servicemesh/comprehensive-service-architecture/"&gt;such pattern that specifically&lt;/a&gt; examines the relationship between service mesh and API management to help you understand how to use these solutions together to establish a comprehensive service management architecture.&lt;/p&gt; &lt;p&gt;Interested in trying this on your own? Red Hat Developer's &lt;a href="https://redhat-solution-patterns.github.io/solution-pattern-apim-servicemesh/comprehensive-service-architecture/03-demo.html"&gt;solutions patterns topic page&lt;/a&gt; provides necessary scripts and detailed steps to see the architecture in action.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Service mesh and API management are both powerful tools that can bring significant benefits to your microservices architecture. When used together, they can provide a comprehensive solution for managing and securing your microservices and APIs, improving security, performance, observability, and developer experience. By using both, your organization can have a more holistic view of its microservices and API usage, which can help you make better decisions for future scaling and security.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/01/why-service-mesh-and-api-management-are-better-together" title="Why service mesh and API management are better together"&gt;Why service mesh and API management are better together&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Vamsi Ravula</dc:creator><dc:date>2023-05-01T07:00:00Z</dc:date></entry><entry><title type="html">Monitoring Made Easy: How to Use Micrometer API</title><link rel="alternate" href="https://www.mastertheboss.com/eclipse/eclipse-microservices/monitoring-made-easy-how-to-use-micrometer-api/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/eclipse/eclipse-microservices/monitoring-made-easy-how-to-use-micrometer-api/</id><updated>2023-04-28T14:42:58Z</updated><content type="html">Micrometer is a metrics instrumentation library for Java applications. It provides a simple facade over the instrumentation clients for a number of popular monitoring systems. In this tutorial, we will learn how to use Micrometer API in a Jakarta EE application with WildFly or a Quarkus application. Firstly, we will cover the Micrometer configuration using ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Support for TLS/SSL in TCP</title><link rel="alternate" href="http://belaban.blogspot.com/2023/04/support-for-tlsssl-in-tcp.html" /><author><name>Bela Ban</name></author><id>http://belaban.blogspot.com/2023/04/support-for-tlsssl-in-tcp.html</id><updated>2023-04-28T07:37:00Z</updated><content type="html">In version 5.2.15 (to be released soon), TLS can be enabled in TCP via a simple configuration change: &lt;TCP      tls.enabled="true"      tls.client_auth="NEED"      tls.keystore_path="good-server.jks"      tls.keystore_password="password"      tls.keystore_alias="server" ... /&gt; This installs an SSLSocketFactory into TCP, creating SSLSockets instead of Sockets and SSLServerSockets instead of ServerSockets.   This is an alternative to SYM_ENCRYPT.   Details can be found in [1].   Cheers, [1]</content><dc:creator>Bela Ban</dc:creator></entry><entry><title>How to add public Ingress to a PrivateLink ROSA cluster</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/27/how-add-public-ingress-private-link-rosa-cluster" /><author><name>Suresh Gaikwad</name></author><id>fb947412-f1ec-40f9-8910-2b5dec129b87</id><updated>2023-04-27T07:00:00Z</updated><published>2023-04-27T07:00:00Z</published><summary type="html">&lt;p&gt;This article demonstrates how to expose applications to the internet by deploying in a PrivateLink &lt;a href="https://developers.redhat.com/products/red-hat-openshift-service-on-aws/overview"&gt;Red Hat OpenShift Service on AWS&lt;/a&gt; (ROSA) cluster within a truly private &lt;a href="https://aws.amazon.com/vpc/"&gt;Virtual Private Cloud &lt;/a&gt;(VPC) that doesn’t have a network address translation (NAT) gateway or an internet gateway attached to it. We will be using a single VPC for Ingress and Egress traffic. However, you might choose to have separate VPCs for Ingress and Egress traffic to provide more security control of this traffic.&lt;/p&gt; &lt;p&gt;The cluster used in this article has been installed as per the architecture diagram detailed in Figure 1 of the previous article, &lt;a href="https://developers.redhat.com/articles/2022/04/27/create-privatelink-red-hat-openshift-cluster-aws-sts"&gt;Create a PrivateLink Red Hat OpenShift cluster on AWS with STS&lt;/a&gt;. There are &lt;a href="https://github.com/aws-samples/rosa-patterns/blob/main/templates/cloudformation/privatelink/README.md"&gt;cloud formation templates&lt;/a&gt; to automate the cluster deployment as per the architecture discussed in that previous article.&lt;/p&gt; &lt;p&gt;To expose the applications to the internet, we will use a custom domain operator. The Custom Domains Operator sets up a new Ingress controller with a custom certificate as a day-2 operation. It will create an additional private network load balancer in the ROSA private VPC. We will create an additional network load balancer in the egress VPC and then use the network load balancer created in the ROSA VPC as its target. Additionally, we will create CloudFront distribution for content delivery and WAF to protect web applications by filtering and monitoring HTTP traffic between a web application and the internet. We will use the AWS network firewall for fine-grained control over network traffic.&lt;/p&gt; &lt;p&gt;In this example, we will use a single availability zone for the network firewall in the egress VPC (Figure 1). However, it is strongly recommended that a production cluster uses multiple availability zones to minimize the potential for outages.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/arch_0.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/arch_0.jpg?itok=cZ6eJ9gu" width="1179" height="642" alt="A diagram of the OpenShift Service on AWS architecture." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: OpenShift Service on AWS architecture to expose applications deployed in truly private VPC to the Internet in a secure way.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The network details used in this architecture are:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Egress/Ingress VPC &lt;strong&gt;(10.0.0.0/16)&lt;/strong&gt; &lt;ol&gt;&lt;li aria-level="2"&gt;Egress public subnet &lt;strong&gt;(10.0.128.0/19)&lt;/strong&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;Egress public subnet &lt;strong&gt;(10.0.0.0/17)&lt;/strong&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;Firewall subnet &lt;strong&gt;(10.0.192.0/27)&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;ROSA VPC &lt;strong&gt;(10.1.0.0/16)&lt;/strong&gt; &lt;ol&gt;&lt;li aria-level="2"&gt;ROSA subnet 1 &lt;strong&gt;(10.1.0.0/18)&lt;/strong&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;ROSA subnet 2 &lt;strong&gt;(10.1.64.0/18)&lt;/strong&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;ROSA subnet 3 &lt;strong&gt;(10.1.128.0/17)&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The AWS network firewall will be created in the firewall subnet which will specify the VPC endpoint in the route table entries shown in Figure 2.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rt_0.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/rt_0.jpg?itok=1wiRJDvO" width="1030" height="702" alt="Route tables created to route the traffic to the internet." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: Route tables created to route the traffic to the Internet.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Log in to the jump host from where you have access to the ROSA cluster.&lt;/p&gt; &lt;h2&gt;Configure custom domain for applications&lt;/h2&gt; &lt;p&gt;To configure a custom domain for applications, get the certificates for your custom domain. If you don’t have one, you can generate the certificates using &lt;code&gt;certbot&lt;/code&gt; as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-project custom-domain $ EMAIL=testuser@gmail.com $ DOMAIN=sgaikwad.mobb.ninja $ certbot certonly --manual --preferred-challenges=dns --email $EMAIL --server https://acme-v02.api.letsencrypt.org/directory --agree-tos --manual-public-ip-logging-ok -d "*.$DOMAIN"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Follow the instructions to create a TXT record in route53. This will create certs in &lt;code&gt;/etc/letsencrypt/live/sgaikwad.mobb.cloud-*&lt;/code&gt; directory. Specify the exact path as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ CERTS=/etc/letsencrypt/live/sgaikwad.mobb.cloud-* $ oc create secret tls acme-tls --cert=$CERTS/fullchain.pem --key=$CERTS/privkey.pem $ cat cr.yaml apiVersion: managed.openshift.io/v1alpha1 kind: CustomDomain metadata: name: acme spec: domain: sgaikwad.mobb.cloud scope: Internal loadBalancerType: NLB certificate: name: acme-tls namespace: custom-domain $ oc apply -f cr.yaml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Wait until customdomain has endpoints.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ watch oc get customdomains&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will create a network load balancer in the ROSA VPC. Let's create a network load balancer in Egress/Ingress VPC now.&lt;/p&gt; &lt;p&gt;Create a target group as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export ROSA_EGRESS_VPC=&lt;VpcId_for_Egress_VPC&gt; $ export TG_NAME=rosa-tg1 $ export ROSA_EGRESS_VPC=`aws ec2 describe-vpcs --filters Name=tag:Name,Values=rosa-egress-vpc |jq -r.Vpcs[0].VpcId` $ export TG_ARN=$(aws elbv2 create-target-group --name test --protocol TCP --port 80 --vpc-id $ROSA_EGRESS_VPC --target-type ip --ip-address-type ipv4 --query 'TargetGroups[0].TargetGroupArn' --output text)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To get the &lt;code&gt;target_Id&lt;/code&gt; from your OpenShift Service on AWS cluster, enter the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export NLB_PRIVATE_IP=`nslookup $(oc get customdomains |awk '{print $2}' |tail -1) |grep Address: |grep -v '#' |awk '{print $2}'` &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Register targets for your &lt;strong&gt;Target Group&lt;/strong&gt; as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws elbv2 register-targets --target-group-arn $TG_ARN --targets AvailabilityZone=all,Id=$NLB_PRIVATE_IP,Port=80 $ aws elbv2 register-targets --target-group-arn $TG_ARN --targets AvailabilityZone=all,Id=$NLB_PRIVATE_IP,Port=443&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, create a network load balancer in Egress/Ingress VPC by grabbing the public subnet IDs from the VPC connected to the internet and map it while creating the load balancer as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export SUBNET_ID=`aws ec2 describe-subnets --filters "Name=vpc-id,Values=$ROSA_EGRESS_VPC" --filters "Name=tag:Name,Values=rosa-egress-public-subnet" --query 'Subnets[*].SubnetId' --output text` $ export NLB_ARN=`aws elbv2 create-load-balancer --name sgaikwad-rosa-nlb --type network --scheme internet-facing --subnet-mappings SubnetId=$SUBNET_ID --query 'LoadBalancers[0].LoadBalancerArn' --output text` &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, let's create a listener and default action as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ aws elbv2 wait load-balancer-available --load-balancer-arns $NLB_ARN &amp;&amp; export NLB_LISTENER=$(aws elbv2 create-listener --load-balancer-arn ${NLB_ARN} --port 80 --protocol TCP --default-actions Type=forward,TargetGroupArn=${TG_ARN} --query 'Listeners[0].ListenerArn' --output text) $ aws elbv2 wait load-balancer-available --load-balancer-arns $NLB_ARN &amp;&amp; export NLB_LISTENER=$(aws elbv2 create-listener --load-balancer-arn ${NLB_ARN} --port 443 --protocol TCP --default-actions Type=forward,TargetGroupArn=${TG_ARN} --query 'Listeners[0].ListenerArn' --output text)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add CNAME record in route53 to point to the newly created internet-facing NLB by grabbing the DNS name for the newly created NLB as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export NLB_DNS_NAME=`aws elbv2 describe-load-balancers --load-balancer-arns $NLB_ARN --query 'LoadBalancers[0].DNSName' --output text`&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Go to route53 and select your domain by following these steps. (In my case, it’s mobb.cloud.)&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Click on &lt;strong&gt;Create Record&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Under record name, specify your domain:  &lt;code&gt;*.sgaikwad.mobb.cloud&lt;/code&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Select &lt;strong&gt;CNAME&lt;/strong&gt; for record type.&lt;/li&gt; &lt;li aria-level="1"&gt;In the value field, enter the value: &lt;code&gt;$NLB_DNS_NAME&lt;/code&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Click &lt;strong&gt;Create Record&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;How to implement AWS network firewall&lt;/h2&gt; &lt;p&gt;With the network firewall, you can filter traffic at the perimeter of your VPC. This includes filtering traffic moving to and from an internet gateway, NAT gateway, or over a VPN or AWS Direct Connect. To create a firewall via the console, you need to create a separate subnet where you want to deploy the AWS network firewall. We recommend creating a separate subnet for the firewall because the AWS network firewall can’t inspect the packets originating and targeting from the same subnet. Hence, if you have any other EC2 instances running in the same subnet, the packets won’t be inspected by the AWS network firewall.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ EGRESS_FIREWALL_SUBNET=`aws ec2 create-subnet --vpc-id $ROSA_EGRESS_VPC --cidr-block 10.0.192.0/27 | jq -r.Subnet.SubnetId` $ aws ec2 create-tags --resources $EGRESS_FIREWALL_SUBNET --tags Key=Name,Value=firewall-subnet&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Create a firewall rule group&lt;/h3&gt; &lt;p&gt;Create a firewall rule group that defines what actions to perform on the packets which will be inspected by the firewall. In this example, we will create a stateless firewall rule group.&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Sign in to the AWS Management Console and open the Amazon VPC console at&lt;a href="https://console.aws.amazon.com/vpc/"&gt; https://console.aws.amazon.com/vpc/&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;In the navigation pane, under &lt;strong&gt;Network Firewall&lt;/strong&gt;, choose &lt;strong&gt;Network Firewall rule groups&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Choose &lt;strong&gt;Create Network Firewall rule group&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;On the &lt;strong&gt;Create Network Firewall rule group&lt;/strong&gt; page, for the &lt;strong&gt;Rule group type&lt;/strong&gt;, choose &lt;strong&gt;Stateless rule group&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Enter a name and description for the rule group. You'll use these to identify the rule group when you manage and use it.&lt;/li&gt; &lt;li aria-level="1"&gt;For &lt;strong&gt;Capacity&lt;/strong&gt;, set the maximum capacity you want to allow for the stateless rule group, up to the maximum of 30,000. You can't change this setting after you create the rule group. For information about how to calculate this, refer to:&lt;a href="https://docs.aws.amazon.com/network-firewall/latest/developerguide/rule-group-managing.html#nwfw-rule-group-capacity"&gt; Setting rule group capacity in AWS Network Firewall&lt;/a&gt;. Set the capacity to &lt;strong&gt;10.&lt;/strong&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Review the rules that you want to add to the stateless rule group. Click on &lt;strong&gt;Add rule&lt;/strong&gt;. We will add a rule to allow SSH traffic from your IP and block it for all others as an example. &lt;ol&gt;&lt;li aria-level="2"&gt;Specify the priority to &lt;strong&gt;1&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;Choose &lt;strong&gt;TCP and UDP&lt;/strong&gt; under protocol.&lt;/li&gt; &lt;li aria-level="2"&gt;Under source, select any IPV4 address and enter your public IP address. For source port range, select &lt;strong&gt;Any&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;Select &lt;strong&gt;Custom&lt;/strong&gt; in the destination field and specify 0.0.0.0/0 for destination. Specify port 22 in the destination port range.&lt;/li&gt; &lt;li aria-level="2"&gt;Under actions, select &lt;strong&gt;Pass and create the rule&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;Repeat the above steps to block SSH traffic to everyone else. Specify priority 2 and 0.0.0.0/0 in the source field and &lt;strong&gt;Drop&lt;/strong&gt; in the actions field.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Review the settings for the rule group, then choose &lt;strong&gt;Create stateless rule group&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;h3&gt;Create a firewall policy&lt;/h3&gt; &lt;p&gt;Create a firewall policy by following these steps:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Sign in to the AWS Management Console and open the Amazon VPC console at&lt;a href="https://console.aws.amazon.com/vpc/"&gt; https://console.aws.amazon.com/vpc/&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;In the navigation pane, under &lt;strong&gt;Network Firewall&lt;/strong&gt;, choose &lt;strong&gt;Firewall policies&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Choose &lt;strong&gt;Create firewall policy.&lt;/strong&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Enter a &lt;strong&gt;Name&lt;/strong&gt; to identify this firewall policy.&lt;/li&gt; &lt;li aria-level="1"&gt;For &lt;strong&gt;Stream exception policy&lt;/strong&gt;, choose how the network firewall handles traffic when a network connection breaks midstream. Network connections can break due to disruptions in external networks or within the firewall itself. Choose &lt;strong&gt;Continue&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Choose &lt;strong&gt;Next&lt;/strong&gt; to go to the firewall policy's &lt;strong&gt;Add rule groups&lt;/strong&gt; page.&lt;/li&gt; &lt;li aria-level="1"&gt;Keep the settings at default and click on &lt;strong&gt;Add Stateless rule groups&lt;/strong&gt;. &lt;ol&gt;&lt;li aria-level="2"&gt;Select the rule you created earlier and click on &lt;strong&gt;Add rule Group&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Keep all other settings at default and create the firewall policy.&lt;/li&gt; &lt;/ol&gt;&lt;h3&gt;Create the network firewall&lt;/h3&gt; &lt;p&gt;Once the firewall policy is created, create the network firewall&lt;strong&gt; &lt;/strong&gt;as follows:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Sign in to the AWS Management Console and open the Amazon VPC console at&lt;a href="https://console.aws.amazon.com/vpc/"&gt; https://console.aws.amazon.com/vpc/&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;In the navigation pane, under &lt;strong&gt;Network Firewall&lt;/strong&gt;, choose &lt;strong&gt;Firewalls&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Choose &lt;strong&gt;Create firewall&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Enter a name to identify this firewall.&lt;/li&gt; &lt;li aria-level="1"&gt;Choose your VPC from the dropdown list. Here, choose &lt;strong&gt;ROSA Egress VPC&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;For &lt;strong&gt;Firewall subnets&lt;/strong&gt;, choose the availability zones and subnets that you want to use for your firewall endpoints. Select the newly created Firewall subnet from the list.&lt;/li&gt; &lt;li aria-level="1"&gt;For the &lt;strong&gt;Associated firewall policy&lt;/strong&gt; section, choose the firewall policy that you want to associate with the firewall. Select the firewall policy we created in the previous step.&lt;/li&gt; &lt;li aria-level="1"&gt;Select &lt;strong&gt;Create firewall&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;h3&gt;Configure VPC route tables&lt;/h3&gt; &lt;p&gt;Now, configure your VPC route tables to send traffic through the firewall endpoints. For more information, refer to&lt;a href="https://docs.aws.amazon.com/network-firewall/latest/developerguide/vpc-config.html#vpc-config-route-tables"&gt; VPC route table configuration for AWS Network Firewall&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Create a new route table to route all the incoming and outgoing traffic through firewall. Under VPC, from the route tables page, click &lt;strong&gt;Create Route Table&lt;/strong&gt;.&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Specify a name for your route table (i.e., FirewallRT).&lt;/li&gt; &lt;li aria-level="1"&gt;Select the Egress VPC and click on &lt;strong&gt;create Route table&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Select the newly created subnet and click on subnet associations from the bottom menu.&lt;/li&gt; &lt;li aria-level="1"&gt;Click on &lt;strong&gt;Edit subnet association&lt;/strong&gt; and select &lt;strong&gt;Firewall Subnet&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Then, click on the &lt;strong&gt;Routes&lt;/strong&gt; and add two routes as follows: &lt;ol&gt;&lt;li aria-level="2"&gt;For destination 0.0.0.0/0, the target would be the Internet gateway associated with Egress VPC.&lt;/li&gt; &lt;li aria-level="2"&gt;For destination 10.0.0.0/16, the target would be local.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Similarly, create a new route table for the internet gateway. Specify the name InternetGwRT.&lt;/li&gt; &lt;li aria-level="1"&gt;Select Egress VPC and click on &lt;strong&gt;create route table&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Select the InternetGwRT from the list and click on &lt;strong&gt;Edge Associations&lt;/strong&gt;. &lt;ol&gt;&lt;li aria-level="2"&gt;Attach the internet gateway by clicking on &lt;strong&gt;Edit Edge Associations&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Click on the routes and add the following two routes: &lt;ol&gt;&lt;li aria-level="2"&gt;For destination 10.0.128.0/19, the target would be VPC endpoint which was created after creating the network firewall. Verify the VPC endpoint for the firewall from the endpoints section under VPC.&lt;/li&gt; &lt;li aria-level="2"&gt;For destination 10.0.0.0/16, the target would be local.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Now, select the rosa-egress-public-rt route table and verify the following three routes. If any routes are missing, create those. &lt;ol&gt;&lt;li aria-level="2"&gt;For destination 0.0.0.0/0, the target would be VPC endpoint which was created after creating the Network firewall.&lt;/li&gt; &lt;li aria-level="2"&gt;For destination 10.0.0.0/16, the target would be local.&lt;/li&gt; &lt;li aria-level="2"&gt;For destination 10.1.0.0/16, the target would be transit gateway (rosa-egress-tgw-attachment).&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;/ol&gt;&lt;h3&gt;Create WAF and cloudfront distribution&lt;/h3&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Create a WAF rule here:&lt;a href="https://console.aws.amazon.com/wafv2/homev2/web-acls/new?region=us-east-2"&gt; https://console.aws.amazon.com/wafv2/homev2/web-acls/new?region=us-east-2&lt;/a&gt; &lt;ol&gt;&lt;li aria-level="2"&gt;Specify the name for your web ACL.&lt;/li&gt; &lt;li aria-level="2"&gt;Select &lt;strong&gt;Amazon CloudFront distributions&lt;/strong&gt; in the resource type and click &lt;strong&gt;next&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;Click on &lt;strong&gt;Add rules&lt;/strong&gt; and select &lt;strong&gt;Add Managed Rule groups&lt;/strong&gt; from the list.&lt;/li&gt; &lt;li aria-level="2"&gt;Use the core rule set and SQL injection rules from the free rule groups.&lt;/li&gt; &lt;li aria-level="2"&gt;Click &lt;strong&gt;Review and Create&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Add a certificate to ACM: &lt;a href="https://us-east-2.console.aws.amazon.com/acm/home?region=us-east-1#/importwizard/"&gt;https://us-east-2.console.aws.amazon.com/acm/home?region=us-east-1#/importwizard/&lt;/a&gt; &lt;ol&gt;&lt;li aria-level="2"&gt;Click on import and paste in the cert, key, certchain from the files which we have generated earlier for the custom domain. Click &lt;strong&gt;Review and Create&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Log into the&lt;a href="https://us-east-1.console.aws.amazon.com/cloudfront/v3/home#/distributions"&gt; AWS console and Create a Cloud Front distribution&lt;/a&gt; (make sure it's the same region as your cluster). &lt;ol&gt;&lt;li aria-level="2"&gt;Click on &lt;strong&gt;Create distribution&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;In &lt;strong&gt;Origin Domain Name&lt;/strong&gt;, select &lt;strong&gt;sgaikwad-rosa-nlb &lt;/strong&gt;(the network load balancer you created in Egress VPC).&lt;/li&gt; &lt;li aria-level="2"&gt;Select &lt;strong&gt;Origin Protocol Policy: HTTPS only&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;For &lt;strong&gt;Enable Origin Shield&lt;/strong&gt;, click on &lt;strong&gt;yes&lt;/strong&gt;. Specify &lt;strong&gt;ap-south-1&lt;/strong&gt; as origin shield region.&lt;/li&gt; &lt;li aria-level="2"&gt;Set the viewer protocol policy to &lt;strong&gt;Redirect HTTP to HTTPS&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;Set the &lt;strong&gt;Allowed HTTP Methods&lt;/strong&gt; to &lt;strong&gt;GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;Under &lt;strong&gt;Cache key and origin requests&lt;/strong&gt;,&lt;strong&gt; &lt;/strong&gt;for Cache Policy, select &lt;strong&gt;CachingDisabled&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;For the &lt;strong&gt;Origin request policy&lt;/strong&gt;, click on &lt;strong&gt;Create Policy &lt;/strong&gt;and specify the name for your policy.&lt;/li&gt; &lt;li aria-level="2"&gt;Under &lt;strong&gt;Origin request settings&lt;/strong&gt;, select &lt;strong&gt;Allow viewer headers, All query string and all cookies&lt;/strong&gt;. Click on &lt;strong&gt;Create&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;Select &lt;strong&gt;sgaikwad&lt;/strong&gt; in AWS WAF web ACL (the web ACL which we created in previous step).&lt;/li&gt; &lt;li aria-level="2"&gt;Under &lt;strong&gt;Alternate domain name (CNAME)&lt;/strong&gt;, click on &lt;strong&gt;Add item&lt;/strong&gt; and use &lt;strong&gt;*.sgaikwad.mobb.cloud&lt;/strong&gt; as a domain. Replace this with your domain.&lt;/li&gt; &lt;li aria-level="2"&gt;Under &lt;strong&gt;Custom SSL certificate&lt;/strong&gt;, select the domain you uploaded the certificate in ACM.&lt;/li&gt; &lt;li aria-level="2"&gt;Click on &lt;strong&gt;create distribution&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;Under the details page for your distribution, check &lt;strong&gt;Distribution domain name&lt;/strong&gt; and copy it.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Modify the route53 CNAME record to point to the cloud front distribution instead of Egress NLB. &lt;ol&gt;&lt;li aria-level="2"&gt;Under services, go to Route53 hosted zones.&lt;/li&gt; &lt;li aria-level="2"&gt;Select your hosted zone. For me, it’s mobb.cloud.&lt;/li&gt; &lt;li aria-level="2"&gt;Select *.sgaikwad.mobb.cloud CNAME record, click on &lt;strong&gt;Edit&lt;/strong&gt; and change the value to the Cloudfront distribution domain name value that you copied in the previous step.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Deploy the application&lt;/h2&gt; &lt;p&gt;Now that we have configured all the components, we are all set to deploy the sample application in the ROSA cluster and access it from the internet by creating a route with the domain name.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-project application $ oc new-app --docker-image=docker.io/openshift/hello-openshift $ oc create route edge --service=hello-openshift hello-openshift-tls --hostname hello-openshift.sgaikwad.mobb.cloud&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, open the browser from your local system and access the application with &lt;a href="https://hello-openshift.sgaikwad.mobb.cloud"&gt;https://hello-openshift.sgaikwad.mobb.cloud&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Learn more&lt;/h2&gt; &lt;p&gt;If you have questions, please comment below. We welcome your feedback. Visit &lt;a href="https://developers.redhat.com/products/red-hat-openshift-service-on-aws/overview"&gt;Red Hat OpenShift Services on AWS&lt;/a&gt; (ROSA) for more information and resources.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/27/how-add-public-ingress-private-link-rosa-cluster" title="How to add public Ingress to a PrivateLink ROSA cluster"&gt;How to add public Ingress to a PrivateLink ROSA cluster&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Suresh Gaikwad</dc:creator><dc:date>2023-04-27T07:00:00Z</dc:date></entry><entry><title type="html">Using Dashbuilder with Google Spreadsheets</title><link rel="alternate" href="https://blog.kie.org/2023/04/using-dashbuilder-with-google-spreadsheets.html" /><author><name>William Siqueira</name></author><id>https://blog.kie.org/2023/04/using-dashbuilder-with-google-spreadsheets.html</id><updated>2023-04-26T17:04:45Z</updated><content type="html">can read from JSON and CSV contents. The only requirement is to make the file accessible to the machine where Dashbuilder is running. The issues you may face are: * : To solve CORS issue you need to use a web proxy to access the service or correctly configure CORS headers; * : To solve this you can have a backend with authentication setup or include headers in the dataset declaration - uuid: my_dataset   url: http://acme.org/myfile.csv       headers:   Authorization: Bearer {token} For Google spreadsheets we don’t have to worry about CORS or authentication as long as the document is published on the internet.  STEPS TO READ A GOOGLE SPREADSHEET  FROM DASHBUILDER * Publish the document on the internet: The document must be public on the internet, so make it sure that you publish it with the option Anyone with the link. Here’s how it looks like in Portuguese: * Now you need the sheet ID. It is what will allow us to get the same sheet output in CSV format. In our example here’s the ID: * Use the ID on this URL template. You can also specify a sheet name if you want In our example here’s the final URL: * Now you can use the sheet from Dashbuilder. Just remember that for CSVs the first row is skipped. Here’s a sample dashboard which you can use to get started: properties: sheet_id: 1XuyPTyrjMFXQ1ey6Bg9AEcrpwZ60CnLQVEs4-DEDrcc datasets:    - uuid: sheet      url: https://docs.google.com/spreadsheets/d/${sheet_id}/gviz/tq?tqx=out:csv pages:    - components:          - settings:                type: BARCHART                lookup:                    uuid: sheet    You can edit this same example using our . CONCLUSION Dashbuilder can easily integrate with Google Spreadsheet and other documents available on the internet. Stay tuned for more Dashbuilder tutorials and articles! The post appeared first on .</content><dc:creator>William Siqueira</dc:creator></entry><entry><title>Quarkus 3.0, our new major release, is here!</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-3-0-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-3-0-final-released/</id><updated>2023-04-26T00:00:00Z</updated><published>2023-04-26T00:00:00Z</published><summary type="html">Quarkus 3.0 is the result of a lot of work and dedication from the community. Quarkus continues to be an Open Source stack to write Java applications offering unparalleled startup time, memory footprint, and developer experience. The development of Quarkus 3 started last year on March 18, 2022, with the...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2023-04-26T00:00:00Z</dc:date></entry><entry><title>Optimize container images for NGINX and Apache HTTPd</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/04/25/optimize-container-images-nginx-and-apache-httpd" /><author><name>Honza Horak</name></author><id>40488572-2eb5-4d73-bbf3-95f8210de674</id><updated>2023-04-25T07:00:00Z</updated><published>2023-04-25T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/containers/"&gt;Container&lt;/a&gt; image size matters. Let’s look at an experiment that reduced Apache HTTP and NGINX servers to micro container images. This article walks through the process we used to achieve the final result, plus how many megabytes (MB) this approach saved.&lt;/p&gt; &lt;p&gt;For this experiment, we used Fedora RPMs, but a similar approach should work in other operating systems or container images, as you see in the list of available images (there is an Apache HTTP server image that uses CentOS Stream 8 and 9 RPMs).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A shortcut:&lt;/strong&gt; If you are only interested in the result and want to try out the micro variant of Apache HTTP or NGINX server container images, check out the images in the following registries.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman pull quay.io/fedora/httpd-24-micro&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman pull quay.io/fedora/nginx-122-micro&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman pull quay.io/sclorg/httpd-24-micro-c8s&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman pull quay.io/sclorg/httpd-24-micro-c9s&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;The benefits of smaller container images&lt;/h2&gt; &lt;p&gt;First, let’s explain a bit more about the story behind those micro containers.&lt;/p&gt; &lt;p&gt;Container images include everything that a specific application needs except a &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; kernel. That's the spirit of container technology. Here, our focus is on web servers, so in addition to the Apache HTTPd and NGINX server daemons, the container needs to include also libraries that those daemons use, necessary userspace components, etc.&lt;/p&gt; &lt;p&gt;Even in the year 2023 when Internet speeds are tremendous, the size of such containers is important. For example, it matters in an environment where the Internet speed is still very limited (have you heard about &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;OpenShift&lt;/a&gt; running on satellites somewhere far in space?). It can help make the user experience more delightful (waiting dozens of seconds is not fun) or limit potential attack vectors because of unnecessary pieces of software in the container image that could work without them.&lt;/p&gt; &lt;p&gt;These are just a few reasons why developers want to make the container image as small as practically possible. Now let's review the steps we took to reduce the web server container image size to a minimum. Our experiments used Apache HTTP server 2.4 and NGINX server 1.22.&lt;/p&gt; &lt;h2&gt;Choosing binaries&lt;/h2&gt; &lt;p&gt;For these tech preview container images, we decided to use a Fedora 36 base image and therefore take RPMs from Fedora repositories. There are different attempts to make the container image small by compiling just the necessary pieces directly from the source, which results in a small image, but that’s not always a good idea.&lt;/p&gt; &lt;p&gt;Using packages from a distribution has a clear benefit—they are well-tested, maintained when there is a security issue, interact well with the rest of the operating system, and are proven to work well outside of the container, so we only need to focus on the container specifics.&lt;/p&gt; &lt;p&gt;You might think about removing files once they are installed as RPMs; this might make the image smaller, but it would be rather risky and a container image could crash in some corner cases when some files would be needed, despite it seemed not like that. If our goal is to create a container image good enough for production, we should follow a simple principle: to not remove files from RPMs, so RPM packages are installed fully or not at all.&lt;/p&gt; &lt;p&gt;However, let's first see what we started with. The container images users currently can use for the latest stable web servers are as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;Container image        | Compressed size | Uncompressed size | Apache HTTP Server 2.4 | 120 MB          | 376 MB            | Nginx 1.22             | 111 MB          | 348 MB            |&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Minimizing the container image&lt;/h2&gt; &lt;p&gt;The main trick is to use a two-phase building of the container image. That means that we use a parent image only for installing RPMs to an empty directory and then use only the content of this directly as a final result. This way we not only get rid of the package installer (DNF) but also the RPM database and RPM tooling itself. We end up with only the web server RPMs and their direct and indirect dependencies.&lt;/p&gt; &lt;p&gt;This change alone already makes a big difference in size, but it also means installing additional software into such an image is not easy. Extending such an image would either mean copying files directly to the image, or the image would need to be rebuilt from scratch. That’s an acceptable disadvantage because, for many use cases, users do not need to extend images with web servers.&lt;/p&gt; &lt;h3&gt;Analyzing dependencies&lt;/h3&gt; &lt;p&gt;The next step was looking closely at what we actually have in the container image. For example, we see &lt;a href="https://developers.redhat.com/cheat-sheets/systemd-commands-cheat-sheet"&gt;systemd&lt;/a&gt; and all of its dependencies. That makes sense when we install the web servers outside of the container image, but in the container? It's likely not needed. So, we worked with Apache HTTPd and NGINX server maintainers, who helped us to get rid of the systemd dependency by installing only &lt;code&gt;httpd-core&lt;/code&gt; and &lt;code&gt;nginx-core&lt;/code&gt; packages. We also avoided installing the Perl module in the case of NGINX, because it pulled in a lot of additional MBs in form of the Perl interpreter and several base libraries.&lt;/p&gt; &lt;p&gt;These changes again helped to squeeze the size significantly. We didn't stop there, though. We analyzed other packages and saw that we installed &lt;code&gt;nss_wrapper&lt;/code&gt; that pulled in the Perl interpreter as well. We also installed &lt;code&gt;gettext&lt;/code&gt; package in order to have &lt;code&gt;envsubst&lt;/code&gt; utility available (for expanding Bash variables in configuration files, as environment variables are common ways to configure container images). In both cases, we worked with the package maintainers, and they allowed us to use only minimal required parts of their tools so we could only install &lt;code&gt;nss_wrapper-libs&lt;/code&gt; and &lt;code&gt;envsubst&lt;/code&gt; packages, which removed additional MBs.&lt;/p&gt; &lt;h3&gt;What we kept in the image&lt;/h3&gt; &lt;p&gt;What we didn't get rid of were several Bash scripts that help the container when starting (starting the daemon, handling the configuration, etc.). These scripts do not take more than a few kilobytes (kB) anyway, so we didn’t touch those.&lt;/p&gt; &lt;p&gt;There are also a couple of other packages that we installed explicitly to make the container images work reasonably (&lt;code&gt;coreutils-single&lt;/code&gt;, &lt;code&gt;glibc-minimal-langpack&lt;/code&gt;), but those were already made as minimal as possible.&lt;/p&gt; &lt;h2&gt;Using the micro web server images&lt;/h2&gt; &lt;p&gt;The container images we worked with are designed to be used either directly via the container command-line interface (&lt;a href="https://developers.redhat.com/articles/podman-next-generation-linux-container-tools/"&gt;Podman&lt;/a&gt; or Docker) in &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;, but they were specifically designed to work well in &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Read more about specific usage in the README files available in the GitHub repositories:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/sclorg/httpd-container/blob/master/2.4-micro/root/usr/share/container-scripts/httpd/README.md"&gt;httpd-container&lt;/a&gt; &lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/sclorg/nginx-container/blob/master/1.22-micro/README.md"&gt;nginx-container&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;The final result&lt;/h2&gt; &lt;p&gt;Did we succeed? Except for the Perl module in the case of the NGINX container image, the tests we have for the images passed fine for the micro container images as well. So, the main use cases should work fine and the micro images should still be pretty useful.&lt;/p&gt; &lt;p&gt;Now we can see how big the micro images are after all those changes:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;Container image              | Compressed size | Uncompressed size | Apache HTTP Server 2.4 micro | 16 MB (13%)     | 46 MB (12%)       | Nginx 1.22 micro             | 23 MB (21%)     | 63 MB (18%)       |&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In summary, we were able to decrease to approximately one-fifth of the original size, so the images will be downloaded five times faster and consume less than one-fifth of space.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The price for such a great difference is not large; the most important feature we lose is the ability to install additional software (due to the missing RPM and DNF). If your use case is to serve static content, then micro HTTPd and NGINX images should do the work without trouble. If your use case is beyond this and you want to serve something complicated or install further RPMs, then the original web server images might be a better choice for you. Or you can create your own micro image, based on the principles explained in this article.&lt;/p&gt; &lt;p&gt;Enjoy the micro web servers, and don't forget to let us know what you think by visiting the GitHub projects below. You can also leave a comment here if you simply like this approach and the images work for you.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/sclorg/nginx-container/issues"&gt;nginx-container&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/sclorg/httpd-container/issues"&gt;httpd-container&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Looking for more? Explore &lt;a href="https://developers.redhat.com/topics/containers/all"&gt;other container tutorials&lt;/a&gt; from Red Hat Developer.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/04/25/optimize-container-images-nginx-and-apache-httpd" title="Optimize container images for NGINX and Apache HTTPd"&gt;Optimize container images for NGINX and Apache HTTPd&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Honza Horak</dc:creator><dc:date>2023-04-25T07:00:00Z</dc:date></entry></feed>
